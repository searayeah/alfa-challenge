{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from autogluon.tabular import TabularPredictor"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "X_train = pd.read_pickle(\"data/features/x_train.pkl\")\n",
        "X_test = pd.read_pickle(\"data/features/x_test.pkl\")\n",
        "\n",
        "\n",
        "with open(\"data/features/selected_features_500.pkl\", \"rb\") as f:\n",
        "    loaded_dict = pickle.load(f)\n",
        "\n",
        "X_train = X_train[loaded_dict[\"selected_features_names\"] + [\"target\", \"client_num\"]]\n",
        "X_test = X_test[loaded_dict[\"selected_features_names\"] + [\"client_num\"]]\n",
        "\n",
        "submission = X_test[[\"client_num\"]]\n",
        "\n",
        "X_train = X_train.drop([\"client_num\"], axis=1)\n",
        "X_test = X_test.drop(\"client_num\", axis=1)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "predictor = TabularPredictor(\n",
        "    label=\"target\",\n",
        "    problem_type=\"regression\",\n",
        "    eval_metric=\"mean_absolute_error\",\n",
        ").fit(X_train, presets=\"best_quality\", time_limit=8000)"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20241216_154816\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.2\n",
            "Python Version:     3.10.14\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Mon Dec  9 13:26:08 UTC 2024\n",
            "CPU Count:          16\n",
            "Memory Avail:       9.00 GB / 15.49 GB (58.1%)\n",
            "Disk Space Avail:   45.69 GB / 468.09 GB (9.8%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 2000s of the 8000s of remaining time (25%).\n",
            "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
            "2024-12-16 18:48:18,675\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\t\tContext path: \"/home/seara/Desktop/Github/alfa-challenge/AutogluonModels/ag-20241216_154816/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Running DyStack sub-fit ...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Beginning AutoGluon training ... Time limit = 1997s\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m AutoGluon will save models to \"/home/seara/Desktop/Github/alfa-challenge/AutogluonModels/ag-20241216_154816/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Train Data Rows:    62222\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Train Data Columns: 500\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Label Column:       target\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Problem Type:       regression\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Preprocessing data ...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Using Feature Generators to preprocess the data ...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tAvailable Memory:                    7943.95 MB\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tTrain Data (Original)  Memory Usage: 174.40 MB (2.2% of available memory)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tStage 1 Generators:\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tStage 2 Generators:\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tStage 3 Generators:\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tStage 4 Generators:\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tStage 5 Generators:\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tUnused Original Features (Count: 2): ['ts_day_mean_mean__agg_linear_trend__attr_\"rvalue\"__chunk_len_50__f_agg_\"min\"', 'ts_day_max_max__agg_linear_trend__attr_\"rvalue\"__chunk_len_50__f_agg_\"min\"']\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\t('int', []) : 2 | ['ts_day_mean_mean__agg_linear_trend__attr_\"rvalue\"__chunk_len_50__f_agg_\"min\"', 'ts_day_max_max__agg_linear_trend__attr_\"rvalue\"__chunk_len_50__f_agg_\"min\"']\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\t('float', [])  : 410 | ['ts_month_sum_sum__mean_abs_change', 'ts_month_min_min__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)', 'ts_month_min_min__quantile__q_0.3', 'ts_month_min_min__quantile__q_0.4', 'ts_month_min_min__quantile__q_0.1', ...]\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\t('int', [])    :  87 | ['ts_month_max_max__sum_of_reoccurring_values', 'ts_week_sum_sum__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"min\"', 'ts_week_sum_sum__number_cwt_peaks__n_1', 'ts_week_sum_sum__number_crossing_m__m_0', 'ts_week_sum_sum__has_duplicate_min', ...]\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\t('object', []) :   1 | ['mcc_code_<lambda_0>']\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\t('category', [])  :   1 | ['mcc_code_<lambda_0>']\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\t('float', [])     : 410 | ['ts_month_sum_sum__mean_abs_change', 'ts_month_min_min__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)', 'ts_month_min_min__quantile__q_0.3', 'ts_month_min_min__quantile__q_0.4', 'ts_month_min_min__quantile__q_0.1', ...]\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\t('int', [])       :  68 | ['ts_month_max_max__sum_of_reoccurring_values', 'ts_week_sum_sum__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"min\"', 'ts_week_sum_sum__number_cwt_peaks__n_1', 'ts_week_sum_sum__number_crossing_m__m_0', 'ts_week_sum_sum__count_below_mean', ...]\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\t('int', ['bool']) :  19 | ['ts_week_sum_sum__has_duplicate_min', 'ts_week_sum_sum__has_duplicate', 'ts_week_sum_sum__large_standard_deviation__r_0.30000000000000004', 'ts_week_std_std__has_duplicate_min', 'ts_week_std_std__has_duplicate', ...]\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t6.6s = Fit runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t498 features in original data used to generate 498 features in processed data.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tTrain Data (Processed) Memory Usage: 170.72 MB (2.1% of available memory)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Data preprocessing and feature engineering runtime = 8.15s ...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m User-specified model hyperparameters to be fit:\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m {\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m }\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1325.39s of the 1988.57s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.5331\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t2.61s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t27.59s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1293.84s of the 1957.02s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m /home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/tabular/models/knn/_knn_loo_variants.py:131: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   y_pred[:, j] = num / denom\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tWarning: Exception caused KNeighborsDist_BAG_L1 to fail during training... Skipping this model.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tInput contains NaN.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Detailed Traceback:\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 273, in _fit\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     self._fit_single(\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 552, in _fit_single\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     model_base.val_score = model_base.score_with_y_pred_proba(y=y, y_pred_proba=self._oof_pred_proba)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1169, in score_with_y_pred_proba\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     return compute_metric(\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/score_func.py\", line 97, in compute_metric\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     return func(y, predictions, **kwargs)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 97, in __call__\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     return self._score(y_true=y_true, y_pred=y_pred, **k)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 135, in _score\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     return self._sign * self._score_func(y_true, y_pred, **kwargs)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     return func(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 216, in mean_absolute_error\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     _assert_all_finite(\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     _assert_all_finite_element_wise(\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m     raise ValueError(msg_err)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m ValueError: Input contains NaN.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1263.17s of the 1926.35s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.29% memory usage per fold, 61.15%/80.00% total).\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=15.29%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(_ray_fit pid=420829)\u001b[0m [1000]\tvalid_set's l1: 1.22602\n",
            "\u001b[36m(_ray_fit pid=421682)\u001b[0m [1000]\tvalid_set's l1: 1.21216\n",
            "\u001b[36m(_ray_fit pid=421949)\u001b[0m [1000]\tvalid_set's l1: 1.23887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2222\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t163.83s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t4.82s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 1095.44s of the 1758.62s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.86% memory usage per fold, 63.45%/80.00% total).\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=15.86%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(_ray_fit pid=422828)\u001b[0m [1000]\tvalid_set's l1: 1.23594\n",
            "\u001b[36m(_ray_fit pid=424013)\u001b[0m [1000]\tvalid_set's l1: 1.24231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2267\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t153.93s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t3.82s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 938.13s of the 1601.31s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2688\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t819.07s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t10.95s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 105.56s of the 768.74s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 28.28% memory usage per fold, 56.56%/80.00% total).\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=28.28%)\n",
            "\u001b[36m(_ray_fit pid=433473)\u001b[0m \tRan out of time, early stopping on iteration 245.\n",
            "\u001b[36m(_ray_fit pid=433853)\u001b[0m \tRan out of time, early stopping on iteration 238.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=434301)\u001b[0m \tRan out of time, early stopping on iteration 256.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=434792)\u001b[0m \tRan out of time, early stopping on iteration 269.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2443\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t90.69s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t0.37s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 10.55s of the 673.73s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tWarning: Model is expected to require 521.9s to train, which exceeds the maximum time limit of 9.3s, skipping model...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tTime limit exceeded... Skipping ExtraTreesMSE_BAG_L1.\n",
            "\u001b[36m(_ray_fit pid=434820)\u001b[0m \tRan out of time, early stopping on iteration 269.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1.70s of the 664.89s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.93% memory usage per fold, 65.86%/80.00% total).\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=32.93%)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
            "\u001b[36m(_ray_fit pid=435380)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -0.7s)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 661.26s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.684, 'LightGBM_BAG_L1': 0.316}\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2208\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t0.11s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 661.14s of the 661.07s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.87% memory usage per fold, 41.74%/80.00% total).\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=20.87%)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2182\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t71.68s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t2.65s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 586.68s of the 586.61s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.09% memory usage per fold, 44.17%/80.00% total).\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=22.09%)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2207\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t59.77s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t2.32s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 524.35s of the 524.28s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 55 due to low time. Expected time usage reduced from 2824.9s -> 523.0s...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2662\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t232.02s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t2.82s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 288.64s of the 288.57s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.92% memory usage per fold, 49.84%/80.00% total).\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=24.92%)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2165\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t116.67s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t0.37s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 169.36s of the 169.29s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 63 due to low time. Expected time usage reduced from 794.3s -> 167.8s...\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2638\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t118.66s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t3.24s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 46.26s of the 46.20s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 40.12% memory usage per fold, 40.12%/80.00% total).\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=8, gpus=0, memory=40.12%)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L2.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 33.19s of the 33.13s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.01% memory usage per fold, 64.02%/80.00% total).\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=32.01%)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.5705\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t49.0s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t4.47s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -19.32s of remaining time.\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L2': 0.833, 'LightGBMXT_BAG_L2': 0.167}\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t-1.2164\t = Validation score   (-mean_absolute_error)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t0.27s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m AutoGluon training complete, total runtime = 2016.56s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 461.8 rows/s (7778 batch size)\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/seara/Desktop/Github/alfa-challenge/AutogluonModels/ag-20241216_154816/ds_sub_fit/sub_fit_ho\")\n",
            "\u001b[36m(_dystack pid=419055)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                     model  score_holdout  score_val          eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0          CatBoost_BAG_L2      -1.235937  -1.216455  mean_absolute_error        8.152682      47.925754  1346.802276                 0.146506                0.370050         116.674702            2       True         10\n",
            "1      WeightedEnsemble_L3      -1.236066  -1.216353  mean_absolute_error        8.421749      50.574871  1418.757409                 0.001517                0.001595           0.273616            3       True         13\n",
            "2        LightGBMXT_BAG_L2      -1.237173  -1.218215  mean_absolute_error        8.273727      50.203226  1301.809091                 0.267550                2.647522          71.681518            2       True          7\n",
            "3          LightGBM_BAG_L2      -1.238092  -1.220653  mean_absolute_error        8.213812      49.874407  1289.899809                 0.207636                2.318703          59.772236            2       True          8\n",
            "4        LightGBMXT_BAG_L1      -1.239240  -1.222219  mean_absolute_error        0.756705       4.817700   163.831438                 0.756705                4.817700         163.831438            1       True          2\n",
            "5      WeightedEnsemble_L2      -1.239722  -1.220831  mean_absolute_error        1.221652       8.641370   317.869455                 0.002137                0.001391           0.112081            2       True          6\n",
            "6          LightGBM_BAG_L1      -1.242747  -1.226707  mean_absolute_error        0.462810       3.822279   153.925936                 0.462810                3.822279         153.925936            1       True          3\n",
            "7          CatBoost_BAG_L1      -1.264584  -1.244318  mean_absolute_error        0.646442       0.367045    90.689139                 0.646442                0.367045          90.689139            1       True          5\n",
            "8     ExtraTreesMSE_BAG_L2      -1.266594  -1.263830  mean_absolute_error        8.342087      50.793264  1348.790597                 0.335910                3.237560         118.663023            2       True         11\n",
            "9   RandomForestMSE_BAG_L2      -1.270322  -1.266223  mean_absolute_error        8.273871      50.380141  1462.144468                 0.267694                2.824437         232.016894            2       True          9\n",
            "10  RandomForestMSE_BAG_L1      -1.290469  -1.268754  mean_absolute_error        1.450925      10.954677   819.070112                 1.450925               10.954677         819.070112            1       True          4\n",
            "11   KNeighborsUnif_BAG_L1      -1.547647  -1.533107  mean_absolute_error        4.689295      27.594003     2.610949                 4.689295               27.594003           2.610949            1       True          1\n",
            "12          XGBoost_BAG_L2      -1.589404  -1.570543  mean_absolute_error        9.177557      52.028213  1279.130202                 1.171380                4.472509          49.002628            2       True         12\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t2034s\t = DyStack   runtime |\t5966s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 5966s\n",
            "AutoGluon will save models to \"/home/seara/Desktop/Github/alfa-challenge/AutogluonModels/ag-20241216_154816\"\n",
            "Train Data Rows:    70000\n",
            "Train Data Columns: 500\n",
            "Label Column:       target\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    6634.05 MB\n",
            "\tTrain Data (Original)  Memory Usage: 196.20 MB (3.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 1): ['ts_week_std_std__has_duplicate']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('int', []) : 1 | ['ts_week_std_std__has_duplicate']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 410 | ['ts_month_sum_sum__mean_abs_change', 'ts_month_min_min__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)', 'ts_month_min_min__quantile__q_0.3', 'ts_month_min_min__quantile__q_0.4', 'ts_month_min_min__quantile__q_0.1', ...]\n",
            "\t\t('int', [])    :  88 | ['ts_month_max_max__sum_of_reoccurring_values', 'ts_week_sum_sum__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"min\"', 'ts_week_sum_sum__number_cwt_peaks__n_1', 'ts_week_sum_sum__number_crossing_m__m_0', 'ts_week_sum_sum__has_duplicate_min', ...]\n",
            "\t\t('object', []) :   1 | ['mcc_code_<lambda_0>']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :   1 | ['mcc_code_<lambda_0>']\n",
            "\t\t('float', [])     : 410 | ['ts_month_sum_sum__mean_abs_change', 'ts_month_min_min__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)', 'ts_month_min_min__quantile__q_0.3', 'ts_month_min_min__quantile__q_0.4', 'ts_month_min_min__quantile__q_0.1', ...]\n",
            "\t\t('int', [])       :  70 | ['ts_month_max_max__sum_of_reoccurring_values', 'ts_week_sum_sum__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"min\"', 'ts_week_sum_sum__number_cwt_peaks__n_1', 'ts_week_sum_sum__number_crossing_m__m_0', 'ts_week_sum_sum__count_below_mean', ...]\n",
            "\t\t('int', ['bool']) :  18 | ['ts_week_sum_sum__has_duplicate_min', 'ts_week_sum_sum__has_duplicate', 'ts_week_sum_sum__large_standard_deviation__r_0.30000000000000004', 'ts_week_std_std__has_duplicate_min', 'ts_week_mean_mean__fft_coefficient__attr_\"angle\"__coeff_7', ...]\n",
            "\t6.6s = Fit runtime\n",
            "\t499 features in original data used to generate 499 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 192.13 MB (2.9% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 7.73s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3971.39s of the 5958.56s of remaining time.\n",
            "\t-1.5333\t = Validation score   (-mean_absolute_error)\n",
            "\t1.76s\t = Training   runtime\n",
            "\t38.64s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3929.57s of the 5916.75s of remaining time.\n",
            "/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/tabular/models/knn/_knn_loo_variants.py:131: RuntimeWarning: invalid value encountered in divide\n",
            "  y_pred[:, j] = num / denom\n",
            "\tWarning: Exception caused KNeighborsDist_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tInput contains NaN.\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
            "    model = self._train_single(**model_fit_kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 273, in _fit\n",
            "    self._fit_single(\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 552, in _fit_single\n",
            "    model_base.val_score = model_base.score_with_y_pred_proba(y=y, y_pred_proba=self._oof_pred_proba)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1169, in score_with_y_pred_proba\n",
            "    return compute_metric(\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/score_func.py\", line 97, in compute_metric\n",
            "    return func(y, predictions, **kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 97, in __call__\n",
            "    return self._score(y_true=y_true, y_pred=y_pred, **k)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 135, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 216, in mean_absolute_error\n",
            "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
            "    _assert_all_finite(\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
            "    _assert_all_finite_element_wise(\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
            "    raise ValueError(msg_err)\n",
            "ValueError: Input contains NaN.\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3887.84s of the 5875.01s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.85% memory usage per fold, 43.70%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=21.85%)\n",
            "\t-1.2238\t = Validation score   (-mean_absolute_error)\n",
            "\t315.13s\t = Training   runtime\n",
            "\t1.52s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 3569.43s of the 5556.60s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.69% memory usage per fold, 45.38%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=22.69%)\n",
            "\t-1.2302\t = Validation score   (-mean_absolute_error)\n",
            "\t182.26s\t = Training   runtime\n",
            "\t0.65s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3383.82s of the 5371.00s of remaining time.\n",
            "\t-1.2715\t = Validation score   (-mean_absolute_error)\n",
            "\t918.09s\t = Training   runtime\n",
            "\t21.85s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2441.67s of the 4428.84s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.10% memory usage per fold, 58.20%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=29.10%)\n",
            "\t-1.2267\t = Validation score   (-mean_absolute_error)\n",
            "\t761.25s\t = Training   runtime\n",
            "\t0.37s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1676.73s of the 3663.90s of remaining time.\n",
            "\t-1.2743\t = Validation score   (-mean_absolute_error)\n",
            "\t423.5s\t = Training   runtime\n",
            "\t12.64s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1238.81s of the 3225.98s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.56% memory usage per fold, 43.12%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=21.56%)\n",
            "\t-1.253\t = Validation score   (-mean_absolute_error)\n",
            "\t395.91s\t = Training   runtime\n",
            "\t2.59s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 839.56s of the 2826.73s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.66% memory usage per fold, 78.65%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=19.66%)\n",
            "\t-1.234\t = Validation score   (-mean_absolute_error)\n",
            "\t250.31s\t = Training   runtime\n",
            "\t8.32s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 585.38s of the 2572.55s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.78% memory usage per fold, 51.14%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=12.78%)\n",
            "\t-1.195\t = Validation score   (-mean_absolute_error)\n",
            "\t233.79s\t = Training   runtime\n",
            "\t8.17s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 347.65s of the 2334.82s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.63% memory usage per fold, 70.50%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=17.63%)\n",
            "\t-1.228\t = Validation score   (-mean_absolute_error)\n",
            "\t261.66s\t = Training   runtime\n",
            "\t1.88s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 82.91s of the 2070.08s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.42% memory usage per fold, 61.68%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=15.42%)\n",
            "\t-1.2418\t = Validation score   (-mean_absolute_error)\n",
            "\t67.48s\t = Training   runtime\n",
            "\t0.29s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 12.50s of the 1999.67s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.84% memory usage per fold, 51.36%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=12.84%)\n",
            "\tTime limit exceeded... Skipping NeuralNetTorch_r79_BAG_L1.\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 397.14s of the 1977.42s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.708, 'LightGBMXT_BAG_L1': 0.208, 'CatBoost_BAG_L1': 0.042, 'LightGBMLarge_BAG_L1': 0.042}\n",
            "\t-1.1892\t = Validation score   (-mean_absolute_error)\n",
            "\t0.21s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1977.18s of the 1976.70s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.02% memory usage per fold, 60.10%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=15.02%)\n",
            "2024-12-16 20:28:45,275\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2024-12-16 20:28:45,291\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "2024-12-16 20:28:45,293\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-1.2179\t = Validation score   (-mean_absolute_error)\n",
            "\t36.13s\t = Training   runtime\n",
            "\t0.47s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1938.29s of the 1937.80s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.08% memory usage per fold, 60.34%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=15.08%)\n",
            "\t-1.2182\t = Validation score   (-mean_absolute_error)\n",
            "\t38.95s\t = Training   runtime\n",
            "\t0.45s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1896.13s of the 1895.65s of remaining time.\n",
            "\t-1.241\t = Validation score   (-mean_absolute_error)\n",
            "\t1042.65s\t = Training   runtime\n",
            "\t24.85s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ... Training model for up to 826.67s of the 826.19s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.28% memory usage per fold, 65.11%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=16.28%)\n",
            "\t-1.2154\t = Validation score   (-mean_absolute_error)\n",
            "\t79.24s\t = Training   runtime\n",
            "\t1.1s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 743.98s of the 743.50s of remaining time.\n",
            "\t-1.24\t = Validation score   (-mean_absolute_error)\n",
            "\t518.59s\t = Training   runtime\n",
            "\t26.41s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 197.22s of the 196.74s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 25.77% memory usage per fold, 51.54%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=25.77%)\n",
            "\t-1.2219\t = Validation score   (-mean_absolute_error)\n",
            "\t158.91s\t = Training   runtime\n",
            "\t2.65s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ... Training model for up to 34.82s of the 34.34s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.95% memory usage per fold, 41.90%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=20.95%)\n",
            "\t-1.5728\t = Validation score   (-mean_absolute_error)\n",
            "\t44.93s\t = Training   runtime\n",
            "\t4.77s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -14.36s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.708, 'LightGBMXT_BAG_L1': 0.167, 'LightGBMLarge_BAG_L1': 0.042, 'CatBoost_BAG_L2': 0.042, 'NeuralNetFastAI_BAG_L2': 0.042}\n",
            "\t-1.1893\t = Validation score   (-mean_absolute_error)\n",
            "\t0.45s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 5981.38s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 732.7 rows/s (8750 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/seara/Desktop/Github/alfa-challenge/AutogluonModels/ag-20241216_154816\")\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "predictor.leaderboard()"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score_val</th>\n",
              "      <th>eval_metric</th>\n",
              "      <th>pred_time_val</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>pred_time_val_marginal</th>\n",
              "      <th>fit_time_marginal</th>\n",
              "      <th>stack_level</th>\n",
              "      <th>can_infer</th>\n",
              "      <th>fit_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>-1.189159</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>11.943610</td>\n",
              "      <td>1572.052587</td>\n",
              "      <td>0.001556</td>\n",
              "      <td>0.213595</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WeightedEnsemble_L3</td>\n",
              "      <td>-1.189271</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>100.674127</td>\n",
              "      <td>4049.754338</td>\n",
              "      <td>0.001581</td>\n",
              "      <td>0.454496</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NeuralNetTorch_BAG_L1</td>\n",
              "      <td>-1.194971</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>8.166085</td>\n",
              "      <td>233.789728</td>\n",
              "      <td>8.166085</td>\n",
              "      <td>233.789728</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CatBoost_BAG_L2</td>\n",
              "      <td>-1.215427</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>98.024514</td>\n",
              "      <td>3890.389241</td>\n",
              "      <td>1.102782</td>\n",
              "      <td>79.235104</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LightGBMXT_BAG_L2</td>\n",
              "      <td>-1.217945</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>97.391894</td>\n",
              "      <td>3847.284231</td>\n",
              "      <td>0.470162</td>\n",
              "      <td>36.130095</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LightGBM_BAG_L2</td>\n",
              "      <td>-1.218166</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>97.368817</td>\n",
              "      <td>3850.107264</td>\n",
              "      <td>0.447085</td>\n",
              "      <td>38.953127</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NeuralNetFastAI_BAG_L2</td>\n",
              "      <td>-1.221920</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>99.569763</td>\n",
              "      <td>3970.064739</td>\n",
              "      <td>2.648031</td>\n",
              "      <td>158.910602</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LightGBMXT_BAG_L1</td>\n",
              "      <td>-1.223831</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>1.522289</td>\n",
              "      <td>315.133953</td>\n",
              "      <td>1.522289</td>\n",
              "      <td>315.133953</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>CatBoost_BAG_L1</td>\n",
              "      <td>-1.226734</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>0.369031</td>\n",
              "      <td>761.253530</td>\n",
              "      <td>0.369031</td>\n",
              "      <td>761.253530</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>LightGBMLarge_BAG_L1</td>\n",
              "      <td>-1.228010</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>1.884649</td>\n",
              "      <td>261.661781</td>\n",
              "      <td>1.884649</td>\n",
              "      <td>261.661781</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>LightGBM_BAG_L1</td>\n",
              "      <td>-1.230228</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>0.651453</td>\n",
              "      <td>182.263284</td>\n",
              "      <td>0.651453</td>\n",
              "      <td>182.263284</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>XGBoost_BAG_L1</td>\n",
              "      <td>-1.233976</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>8.318586</td>\n",
              "      <td>250.311640</td>\n",
              "      <td>8.318586</td>\n",
              "      <td>250.311640</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ExtraTreesMSE_BAG_L2</td>\n",
              "      <td>-1.240017</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>123.332198</td>\n",
              "      <td>4329.739622</td>\n",
              "      <td>26.410466</td>\n",
              "      <td>518.585485</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>RandomForestMSE_BAG_L2</td>\n",
              "      <td>-1.241030</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>121.776688</td>\n",
              "      <td>4853.802239</td>\n",
              "      <td>24.854956</td>\n",
              "      <td>1042.648102</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>CatBoost_r177_BAG_L1</td>\n",
              "      <td>-1.241775</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>0.286577</td>\n",
              "      <td>67.477389</td>\n",
              "      <td>0.286577</td>\n",
              "      <td>67.477389</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NeuralNetFastAI_BAG_L1</td>\n",
              "      <td>-1.253005</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>2.593496</td>\n",
              "      <td>395.913078</td>\n",
              "      <td>2.593496</td>\n",
              "      <td>395.913078</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>RandomForestMSE_BAG_L1</td>\n",
              "      <td>-1.271530</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>21.854260</td>\n",
              "      <td>918.085900</td>\n",
              "      <td>21.854260</td>\n",
              "      <td>918.085900</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>ExtraTreesMSE_BAG_L1</td>\n",
              "      <td>-1.274293</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>12.640072</td>\n",
              "      <td>423.500035</td>\n",
              "      <td>12.640072</td>\n",
              "      <td>423.500035</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>KNeighborsUnif_BAG_L1</td>\n",
              "      <td>-1.533346</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>38.635232</td>\n",
              "      <td>1.763819</td>\n",
              "      <td>38.635232</td>\n",
              "      <td>1.763819</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>XGBoost_BAG_L2</td>\n",
              "      <td>-1.572785</td>\n",
              "      <td>mean_absolute_error</td>\n",
              "      <td>101.694188</td>\n",
              "      <td>3856.079635</td>\n",
              "      <td>4.772456</td>\n",
              "      <td>44.925498</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     model  score_val          eval_metric  pred_time_val  \\\n",
              "0      WeightedEnsemble_L2  -1.189159  mean_absolute_error      11.943610   \n",
              "1      WeightedEnsemble_L3  -1.189271  mean_absolute_error     100.674127   \n",
              "2    NeuralNetTorch_BAG_L1  -1.194971  mean_absolute_error       8.166085   \n",
              "3          CatBoost_BAG_L2  -1.215427  mean_absolute_error      98.024514   \n",
              "4        LightGBMXT_BAG_L2  -1.217945  mean_absolute_error      97.391894   \n",
              "5          LightGBM_BAG_L2  -1.218166  mean_absolute_error      97.368817   \n",
              "6   NeuralNetFastAI_BAG_L2  -1.221920  mean_absolute_error      99.569763   \n",
              "7        LightGBMXT_BAG_L1  -1.223831  mean_absolute_error       1.522289   \n",
              "8          CatBoost_BAG_L1  -1.226734  mean_absolute_error       0.369031   \n",
              "9     LightGBMLarge_BAG_L1  -1.228010  mean_absolute_error       1.884649   \n",
              "10         LightGBM_BAG_L1  -1.230228  mean_absolute_error       0.651453   \n",
              "11          XGBoost_BAG_L1  -1.233976  mean_absolute_error       8.318586   \n",
              "12    ExtraTreesMSE_BAG_L2  -1.240017  mean_absolute_error     123.332198   \n",
              "13  RandomForestMSE_BAG_L2  -1.241030  mean_absolute_error     121.776688   \n",
              "14    CatBoost_r177_BAG_L1  -1.241775  mean_absolute_error       0.286577   \n",
              "15  NeuralNetFastAI_BAG_L1  -1.253005  mean_absolute_error       2.593496   \n",
              "16  RandomForestMSE_BAG_L1  -1.271530  mean_absolute_error      21.854260   \n",
              "17    ExtraTreesMSE_BAG_L1  -1.274293  mean_absolute_error      12.640072   \n",
              "18   KNeighborsUnif_BAG_L1  -1.533346  mean_absolute_error      38.635232   \n",
              "19          XGBoost_BAG_L2  -1.572785  mean_absolute_error     101.694188   \n",
              "\n",
              "       fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
              "0   1572.052587                0.001556           0.213595            2   \n",
              "1   4049.754338                0.001581           0.454496            3   \n",
              "2    233.789728                8.166085         233.789728            1   \n",
              "3   3890.389241                1.102782          79.235104            2   \n",
              "4   3847.284231                0.470162          36.130095            2   \n",
              "5   3850.107264                0.447085          38.953127            2   \n",
              "6   3970.064739                2.648031         158.910602            2   \n",
              "7    315.133953                1.522289         315.133953            1   \n",
              "8    761.253530                0.369031         761.253530            1   \n",
              "9    261.661781                1.884649         261.661781            1   \n",
              "10   182.263284                0.651453         182.263284            1   \n",
              "11   250.311640                8.318586         250.311640            1   \n",
              "12  4329.739622               26.410466         518.585485            2   \n",
              "13  4853.802239               24.854956        1042.648102            2   \n",
              "14    67.477389                0.286577          67.477389            1   \n",
              "15   395.913078                2.593496         395.913078            1   \n",
              "16   918.085900               21.854260         918.085900            1   \n",
              "17   423.500035               12.640072         423.500035            1   \n",
              "18     1.763819               38.635232           1.763819            1   \n",
              "19  3856.079635                4.772456          44.925498            2   \n",
              "\n",
              "    can_infer  fit_order  \n",
              "0        True         12  \n",
              "1        True         20  \n",
              "2        True          9  \n",
              "3        True         16  \n",
              "4        True         13  \n",
              "5        True         14  \n",
              "6        True         18  \n",
              "7        True          2  \n",
              "8        True          5  \n",
              "9        True         10  \n",
              "10       True          3  \n",
              "11       True          8  \n",
              "12       True         17  \n",
              "13       True         15  \n",
              "14       True         11  \n",
              "15       True          7  \n",
              "16       True          4  \n",
              "17       True          6  \n",
              "18       True          1  \n",
              "19       True         19  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "predictions = predictor.predict(X_test)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "submission[\"target\"] = np.clip(predictions, 0, 7)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "submission.to_csv(\"submissions/500_features_autogluon_MAE_unsubmitted.csv\", index=False)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "submission[\"target\"] = submission[\"target\"] - 0.5"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "submission.to_csv(\n",
        "    \"submissions/500_features_autogluon_MAE_unsubmitted-0.5.csv\", index=False\n",
        ")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}
