{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {},
  "cells": [
    {
      "metadata": {},
      "source": [
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from autogluon.tabular import TabularPredictor"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "X_train = pd.read_pickle(\"data/features/x_train.pkl\")\n",
        "X_test = pd.read_pickle(\"data/features/x_test.pkl\")\n",
        "\n",
        "\n",
        "with open(\"data/features/selected_features_500.pkl\", \"rb\") as f:\n",
        "    loaded_dict = pickle.load(f)\n",
        "\n",
        "X_train = X_train[loaded_dict[\"selected_features_names\"] + [\"target\", \"client_num\"]]\n",
        "X_test = X_test[loaded_dict[\"selected_features_names\"] + [\"client_num\"]]\n",
        "\n",
        "submission = X_test[[\"client_num\"]]\n",
        "\n",
        "X_train = X_train.drop([\"client_num\"], axis=1)\n",
        "X_test = X_test.drop(\"client_num\", axis=1)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "predictor = TabularPredictor(\n",
        "    label=\"target\",\n",
        "    problem_type=\"regression\",\n",
        "    eval_metric=\"mean_absolute_percentage_error\",\n",
        ").fit(X_train, presets=\"best_quality\", time_limit=8000)"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20241216_132207\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.2\n",
            "Python Version:     3.10.14\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Mon Dec  9 13:26:08 UTC 2024\n",
            "CPU Count:          16\n",
            "Memory Avail:       11.00 GB / 15.49 GB (71.0%)\n",
            "Disk Space Avail:   49.58 GB / 468.09 GB (10.6%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 2000s of the 8000s of remaining time (25%).\n",
            "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
            "2024-12-16 16:22:10,935\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\t\tContext path: \"/home/seara/Desktop/Github/alfa-challenge/AutogluonModels/ag-20241216_132207/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Running DyStack sub-fit ...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Beginning AutoGluon training ... Time limit = 1995s\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m AutoGluon will save models to \"/home/seara/Desktop/Github/alfa-challenge/AutogluonModels/ag-20241216_132207/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Train Data Rows:    62222\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Train Data Columns: 500\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Label Column:       target\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Problem Type:       regression\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Preprocessing data ...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Using Feature Generators to preprocess the data ...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tAvailable Memory:                    10311.46 MB\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tTrain Data (Original)  Memory Usage: 174.40 MB (1.7% of available memory)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tStage 1 Generators:\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tStage 2 Generators:\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tStage 3 Generators:\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tStage 4 Generators:\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tStage 5 Generators:\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tUnused Original Features (Count: 2): ['ts_day_mean_mean__agg_linear_trend__attr_\"rvalue\"__chunk_len_50__f_agg_\"min\"', 'ts_day_max_max__agg_linear_trend__attr_\"rvalue\"__chunk_len_50__f_agg_\"min\"']\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\t('int', []) : 2 | ['ts_day_mean_mean__agg_linear_trend__attr_\"rvalue\"__chunk_len_50__f_agg_\"min\"', 'ts_day_max_max__agg_linear_trend__attr_\"rvalue\"__chunk_len_50__f_agg_\"min\"']\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\t('float', [])  : 410 | ['ts_month_sum_sum__mean_abs_change', 'ts_month_min_min__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)', 'ts_month_min_min__quantile__q_0.3', 'ts_month_min_min__quantile__q_0.4', 'ts_month_min_min__quantile__q_0.1', ...]\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\t('int', [])    :  87 | ['ts_month_max_max__sum_of_reoccurring_values', 'ts_week_sum_sum__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"min\"', 'ts_week_sum_sum__number_cwt_peaks__n_1', 'ts_week_sum_sum__number_crossing_m__m_0', 'ts_week_sum_sum__has_duplicate_min', ...]\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\t('object', []) :   1 | ['mcc_code_<lambda_0>']\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\t('category', [])  :   1 | ['mcc_code_<lambda_0>']\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\t('float', [])     : 410 | ['ts_month_sum_sum__mean_abs_change', 'ts_month_min_min__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)', 'ts_month_min_min__quantile__q_0.3', 'ts_month_min_min__quantile__q_0.4', 'ts_month_min_min__quantile__q_0.1', ...]\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\t('int', [])       :  68 | ['ts_month_max_max__sum_of_reoccurring_values', 'ts_week_sum_sum__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"min\"', 'ts_week_sum_sum__number_cwt_peaks__n_1', 'ts_week_sum_sum__number_crossing_m__m_0', 'ts_week_sum_sum__count_below_mean', ...]\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\t('int', ['bool']) :  19 | ['ts_week_sum_sum__has_duplicate_min', 'ts_week_sum_sum__has_duplicate', 'ts_week_sum_sum__large_standard_deviation__r_0.30000000000000004', 'ts_week_std_std__has_duplicate_min', 'ts_week_std_std__has_duplicate', ...]\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t6.3s = Fit runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t498 features in original data used to generate 498 features in processed data.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tTrain Data (Processed) Memory Usage: 170.72 MB (1.6% of available memory)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Data preprocessing and feature engineering runtime = 7.16s ...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_percentage_error'\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m User-specified model hyperparameters to be fit:\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m {\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m }\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1325.03s of the 1988.04s of remaining time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t-1856274592444092.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t1.78s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t24.95s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1297.12s of the 1960.13s of remaining time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m /home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/tabular/models/knn/_knn_loo_variants.py:131: RuntimeWarning: invalid value encountered in divide\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   y_pred[:, j] = num / denom\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tWarning: Exception caused KNeighborsDist_BAG_L1 to fail during training... Skipping this model.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t\tInput contains NaN.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Detailed Traceback:\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Traceback (most recent call last):\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 273, in _fit\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     self._fit_single(\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 552, in _fit_single\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     model_base.val_score = model_base.score_with_y_pred_proba(y=y, y_pred_proba=self._oof_pred_proba)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1169, in score_with_y_pred_proba\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     return compute_metric(\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/score_func.py\", line 97, in compute_metric\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     return func(y, predictions, **kwargs)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 97, in __call__\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     return self._score(y_true=y_true, y_pred=y_pred, **k)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 135, in _score\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     return self._sign * self._score_func(y_true, y_pred, **kwargs)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     return func(*args, **kwargs)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 399, in mean_absolute_percentage_error\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     _assert_all_finite(\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     _assert_all_finite_element_wise(\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m   File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m     raise ValueError(msg_err)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m ValueError: Input contains NaN.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1270.18s of the 1933.19s of remaining time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.41% memory usage per fold, 45.63%/80.00% total).\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=11.41%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(_ray_fit pid=339681)\u001b[0m [1000]\tvalid_set's l2: 2.41985\tvalid_set's mean_absolute_percentage_error: -1.52376e+15\n",
            "\u001b[36m(_ray_fit pid=339681)\u001b[0m [2000]\tvalid_set's l2: 2.43429\tvalid_set's mean_absolute_percentage_error: -1.51329e+15\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=339680)\u001b[0m [2000]\tvalid_set's l2: 2.4755\tvalid_set's mean_absolute_percentage_error: -1.44221e+15\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=339681)\u001b[0m [3000]\tvalid_set's l2: 2.44203\tvalid_set's mean_absolute_percentage_error: -1.51156e+15\n",
            "\u001b[36m(_ray_fit pid=339680)\u001b[0m [3000]\tvalid_set's l2: 2.48699\tvalid_set's mean_absolute_percentage_error: -1.43831e+15\n",
            "\u001b[36m(_ray_fit pid=339681)\u001b[0m [4000]\tvalid_set's l2: 2.45161\tvalid_set's mean_absolute_percentage_error: -1.50978e+15\n",
            "\u001b[36m(_ray_fit pid=340888)\u001b[0m [1000]\tvalid_set's l2: 2.48078\tvalid_set's mean_absolute_percentage_error: -1.5344e+15\n",
            "\u001b[36m(_ray_fit pid=339681)\u001b[0m [5000]\tvalid_set's l2: 2.46111\tvalid_set's mean_absolute_percentage_error: -1.50785e+15\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=341533)\u001b[0m [1000]\tvalid_set's l2: 2.45666\tvalid_set's mean_absolute_percentage_error: -1.5056e+15\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=339681)\u001b[0m [6000]\tvalid_set's l2: 2.46869\tvalid_set's mean_absolute_percentage_error: -1.50955e+15\n",
            "\u001b[36m(_ray_fit pid=340888)\u001b[0m [3000]\tvalid_set's l2: 2.50038\tvalid_set's mean_absolute_percentage_error: -1.52231e+15\n",
            "\u001b[36m(_ray_fit pid=341779)\u001b[0m [1000]\tvalid_set's l2: 2.5118\tvalid_set's mean_absolute_percentage_error: -1.54001e+15\n",
            "\u001b[36m(_ray_fit pid=341533)\u001b[0m [2000]\tvalid_set's l2: 2.46641\tvalid_set's mean_absolute_percentage_error: -1.49525e+15\n",
            "\u001b[36m(_ray_fit pid=340888)\u001b[0m [4000]\tvalid_set's l2: 2.51039\tvalid_set's mean_absolute_percentage_error: -1.52181e+15\n",
            "\u001b[36m(_ray_fit pid=341779)\u001b[0m [2000]\tvalid_set's l2: 2.52401\tvalid_set's mean_absolute_percentage_error: -1.52459e+15\n",
            "\u001b[36m(_ray_fit pid=341533)\u001b[0m [3000]\tvalid_set's l2: 2.47346\tvalid_set's mean_absolute_percentage_error: -1.4917e+15\n",
            "\u001b[36m(_ray_fit pid=340888)\u001b[0m [5000]\tvalid_set's l2: 2.51704\tvalid_set's mean_absolute_percentage_error: -1.51964e+15\n",
            "\u001b[36m(_ray_fit pid=341779)\u001b[0m [3000]\tvalid_set's l2: 2.53373\tvalid_set's mean_absolute_percentage_error: -1.51976e+15\n",
            "\u001b[36m(_ray_fit pid=341533)\u001b[0m [4000]\tvalid_set's l2: 2.47856\tvalid_set's mean_absolute_percentage_error: -1.48908e+15\n",
            "\u001b[36m(_ray_fit pid=340888)\u001b[0m [6000]\tvalid_set's l2: 2.52398\tvalid_set's mean_absolute_percentage_error: -1.51991e+15\n",
            "\u001b[36m(_ray_fit pid=341779)\u001b[0m [4000]\tvalid_set's l2: 2.54584\tvalid_set's mean_absolute_percentage_error: -1.51681e+15\n",
            "\u001b[36m(_ray_fit pid=341533)\u001b[0m [5000]\tvalid_set's l2: 2.48359\tvalid_set's mean_absolute_percentage_error: -1.48604e+15\n",
            "\u001b[36m(_ray_fit pid=341779)\u001b[0m [5000]\tvalid_set's l2: 2.55266\tvalid_set's mean_absolute_percentage_error: -1.51472e+15\n",
            "\u001b[36m(_ray_fit pid=341533)\u001b[0m [6000]\tvalid_set's l2: 2.48516\tvalid_set's mean_absolute_percentage_error: -1.48334e+15\n",
            "\u001b[36m(_ray_fit pid=341779)\u001b[0m [6000]\tvalid_set's l2: 2.56165\tvalid_set's mean_absolute_percentage_error: -1.51222e+15\n",
            "\u001b[36m(_ray_fit pid=341533)\u001b[0m [7000]\tvalid_set's l2: 2.48908\tvalid_set's mean_absolute_percentage_error: -1.48331e+15\n",
            "\u001b[36m(_ray_fit pid=341779)\u001b[0m [7000]\tvalid_set's l2: 2.56665\tvalid_set's mean_absolute_percentage_error: -1.51214e+15\n",
            "\u001b[36m(_ray_fit pid=341533)\u001b[0m [8000]\tvalid_set's l2: 2.49301\tvalid_set's mean_absolute_percentage_error: -1.48355e+15\n",
            "\u001b[36m(_ray_fit pid=341779)\u001b[0m [8000]\tvalid_set's l2: 2.56952\tvalid_set's mean_absolute_percentage_error: -1.51223e+15\n",
            "\u001b[36m(_ray_fit pid=341533)\u001b[0m [9000]\tvalid_set's l2: 2.49472\tvalid_set's mean_absolute_percentage_error: -1.4838e+15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t-1481617038722471.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t495.49s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t16.81s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 770.14s of the 1433.15s of remaining time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.69% memory usage per fold, 46.76%/80.00% total).\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=11.69%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(_ray_fit pid=343981)\u001b[0m [1000]\tvalid_set's l2: 2.50606\tvalid_set's mean_absolute_percentage_error: -1.50941e+15\n",
            "\u001b[36m(_ray_fit pid=343981)\u001b[0m [2000]\tvalid_set's l2: 2.51392\tvalid_set's mean_absolute_percentage_error: -1.49879e+15\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=343982)\u001b[0m [2000]\tvalid_set's l2: 2.45915\tvalid_set's mean_absolute_percentage_error: -1.4465e+15\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=344943)\u001b[0m [1000]\tvalid_set's l2: 2.50765\tvalid_set's mean_absolute_percentage_error: -1.55231e+15\n",
            "\u001b[36m(_ray_fit pid=343981)\u001b[0m [3000]\tvalid_set's l2: 2.52749\tvalid_set's mean_absolute_percentage_error: -1.49594e+15\n",
            "\u001b[36m(_ray_fit pid=345282)\u001b[0m [1000]\tvalid_set's l2: 2.45495\tvalid_set's mean_absolute_percentage_error: -1.46016e+15\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=343981)\u001b[0m [4000]\tvalid_set's l2: 2.52942\tvalid_set's mean_absolute_percentage_error: -1.49537e+15\n",
            "\u001b[36m(_ray_fit pid=345637)\u001b[0m [1000]\tvalid_set's l2: 2.47014\tvalid_set's mean_absolute_percentage_error: -1.51478e+15\n",
            "\u001b[36m(_ray_fit pid=345282)\u001b[0m [2000]\tvalid_set's l2: 2.48119\tvalid_set's mean_absolute_percentage_error: -1.4564e+15\n",
            "\u001b[36m(_ray_fit pid=345800)\u001b[0m [1000]\tvalid_set's l2: 2.52668\tvalid_set's mean_absolute_percentage_error: -1.5522e+15\n",
            "\u001b[36m(_ray_fit pid=343981)\u001b[0m [5000]\tvalid_set's l2: 2.53481\tvalid_set's mean_absolute_percentage_error: -1.49481e+15\n",
            "\u001b[36m(_ray_fit pid=345637)\u001b[0m [2000]\tvalid_set's l2: 2.48233\tvalid_set's mean_absolute_percentage_error: -1.50642e+15\n",
            "\u001b[36m(_ray_fit pid=345800)\u001b[0m [2000]\tvalid_set's l2: 2.55473\tvalid_set's mean_absolute_percentage_error: -1.5457e+15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_ray_fit pid=343981)\u001b[0m \tRan out of time, early stopping on iteration 5814. Best iteration is:\n",
            "\u001b[36m(_ray_fit pid=343981)\u001b[0m \t[5006]\tvalid_set's l2: 2.53456\tvalid_set's mean_absolute_percentage_error: -1.49463e+15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(_ray_fit pid=345800)\u001b[0m [3000]\tvalid_set's l2: 2.56639\tvalid_set's mean_absolute_percentage_error: -1.54027e+15\n",
            "\u001b[36m(_ray_fit pid=345800)\u001b[0m [4000]\tvalid_set's l2: 2.56812\tvalid_set's mean_absolute_percentage_error: -1.53526e+15\n",
            "\u001b[36m(_ray_fit pid=345800)\u001b[0m [5000]\tvalid_set's l2: 2.57454\tvalid_set's mean_absolute_percentage_error: -1.53739e+15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t-1493495921914886.0\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t382.09s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t11.7s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 384.07s of the 1047.08s of remaining time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t-1636704272154106.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t653.61s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t15.96s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 376.00s of remaining time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t-1481617038722471.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t0.08s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 375.90s of the 375.40s of remaining time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.22% memory usage per fold, 48.87%/80.00% total).\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=12.22%)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t-1496861682506841.0\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t64.32s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t3.7s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 308.57s of the 308.07s of remaining time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.19% memory usage per fold, 48.77%/80.00% total).\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=12.19%)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t-1498717583006860.0\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t68.08s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t3.72s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 237.14s of the 236.63s of remaining time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t-1566602279703919.0\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t855.45s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t24.14s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -644.54s of remaining time.\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t-1481617038722471.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t0.14s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m AutoGluon training complete, total runtime = 2640.08s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 462.7 rows/s (7778 batch size)\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/seara/Desktop/Github/alfa-challenge/AutogluonModels/ag-20241216_132207/ds_sub_fit/sub_fit_ho\")\n",
            "\u001b[36m(_dystack pid=338096)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                    model  score_holdout     score_val                     eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0       LightGBMXT_BAG_L1  -1.489947e+15 -1.481617e+15  mean_absolute_percentage_error        3.977400      16.808193   495.487050                 3.977400               16.808193         495.487050            1       True          2\n",
            "1     WeightedEnsemble_L3  -1.489947e+15 -1.481617e+15  mean_absolute_percentage_error        3.978897      16.809416   495.625875                 0.001496                0.001223           0.138825            3       True          9\n",
            "2     WeightedEnsemble_L2  -1.489947e+15 -1.481617e+15  mean_absolute_percentage_error        3.979039      16.809323   495.569696                 0.001638                0.001130           0.082646            2       True          5\n",
            "3       LightGBMXT_BAG_L2  -1.494611e+15 -1.496862e+15  mean_absolute_percentage_error       11.302953      73.118179  1597.296345                 0.305295                3.704839          64.323695            2       True          6\n",
            "4         LightGBM_BAG_L2  -1.498845e+15 -1.498718e+15  mean_absolute_percentage_error       11.282193      73.133267  1601.055275                 0.284535                3.719928          68.082625            2       True          7\n",
            "5         LightGBM_BAG_L1  -1.501381e+15 -1.493496e+15  mean_absolute_percentage_error        2.112651      11.696095   382.089494                 2.112651               11.696095         382.089494            1       True          3\n",
            "6  RandomForestMSE_BAG_L2  -1.573015e+15 -1.566602e+15  mean_absolute_percentage_error       11.831716      93.550436  2388.422462                 0.834058               24.137097         855.449813            2       True          8\n",
            "7  RandomForestMSE_BAG_L1  -1.642305e+15 -1.636704e+15  mean_absolute_percentage_error        0.823640      15.963979   653.614286                 0.823640               15.963979         653.614286            1       True          4\n",
            "8   KNeighborsUnif_BAG_L1  -1.866406e+15 -1.856275e+15  mean_absolute_percentage_error        4.083967      24.945071     1.781819                 4.083967               24.945071           1.781819            1       True          1\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t2660s\t = DyStack   runtime |\t5340s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 5340s\n",
            "AutoGluon will save models to \"/home/seara/Desktop/Github/alfa-challenge/AutogluonModels/ag-20241216_132207\"\n",
            "Train Data Rows:    70000\n",
            "Train Data Columns: 500\n",
            "Label Column:       target\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10465.56 MB\n",
            "\tTrain Data (Original)  Memory Usage: 196.20 MB (1.9% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 19 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUnused Original Features (Count: 1): ['ts_week_std_std__has_duplicate']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('int', []) : 1 | ['ts_week_std_std__has_duplicate']\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 410 | ['ts_month_sum_sum__mean_abs_change', 'ts_month_min_min__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)', 'ts_month_min_min__quantile__q_0.3', 'ts_month_min_min__quantile__q_0.4', 'ts_month_min_min__quantile__q_0.1', ...]\n",
            "\t\t('int', [])    :  88 | ['ts_month_max_max__sum_of_reoccurring_values', 'ts_week_sum_sum__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"min\"', 'ts_week_sum_sum__number_cwt_peaks__n_1', 'ts_week_sum_sum__number_crossing_m__m_0', 'ts_week_sum_sum__has_duplicate_min', ...]\n",
            "\t\t('object', []) :   1 | ['mcc_code_<lambda_0>']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  :   1 | ['mcc_code_<lambda_0>']\n",
            "\t\t('float', [])     : 410 | ['ts_month_sum_sum__mean_abs_change', 'ts_month_min_min__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)', 'ts_month_min_min__quantile__q_0.3', 'ts_month_min_min__quantile__q_0.4', 'ts_month_min_min__quantile__q_0.1', ...]\n",
            "\t\t('int', [])       :  70 | ['ts_month_max_max__sum_of_reoccurring_values', 'ts_week_sum_sum__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"min\"', 'ts_week_sum_sum__number_cwt_peaks__n_1', 'ts_week_sum_sum__number_crossing_m__m_0', 'ts_week_sum_sum__count_below_mean', ...]\n",
            "\t\t('int', ['bool']) :  18 | ['ts_week_sum_sum__has_duplicate_min', 'ts_week_sum_sum__has_duplicate', 'ts_week_sum_sum__large_standard_deviation__r_0.30000000000000004', 'ts_week_std_std__has_duplicate_min', 'ts_week_mean_mean__fft_coefficient__attr_\"angle\"__coeff_7', ...]\n",
            "\t6.5s = Fit runtime\n",
            "\t499 features in original data used to generate 499 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 192.13 MB (1.8% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 6.94s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_percentage_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 3554.21s of the 5332.64s of remaining time.\n",
            "\t-1858095138173541.2\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t1.75s\t = Training   runtime\n",
            "\t29.15s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 3522.05s of the 5300.48s of remaining time.\n",
            "/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/tabular/models/knn/_knn_loo_variants.py:131: RuntimeWarning: invalid value encountered in divide\n",
            "  y_pred[:, j] = num / denom\n",
            "\tWarning: Exception caused KNeighborsDist_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\tInput contains NaN.\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
            "    model = self._train_single(**model_fit_kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 273, in _fit\n",
            "    self._fit_single(\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 552, in _fit_single\n",
            "    model_base.val_score = model_base.score_with_y_pred_proba(y=y, y_pred_proba=self._oof_pred_proba)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1169, in score_with_y_pred_proba\n",
            "    return compute_metric(\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/score_func.py\", line 97, in compute_metric\n",
            "    return func(y, predictions, **kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 97, in __call__\n",
            "    return self._score(y_true=y_true, y_pred=y_pred, **k)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/autogluon/core/metrics/__init__.py\", line 135, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 399, in mean_absolute_percentage_error\n",
            "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 113, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n",
            "    _assert_all_finite(\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n",
            "    _assert_all_finite_element_wise(\n",
            "  File \"/home/seara/Desktop/Github/alfa-challenge/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n",
            "    raise ValueError(msg_err)\n",
            "ValueError: Input contains NaN.\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3494.21s of the 5272.64s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.68% memory usage per fold, 50.74%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=12.68%)\n",
            "\t-1483279809548064.2\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t528.75s\t = Training   runtime\n",
            "\t17.34s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2960.12s of the 4738.55s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.71% memory usage per fold, 50.86%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=12.71%)\n",
            "\t-1493011587418404.5\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t405.63s\t = Training   runtime\n",
            "\t10.45s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2550.27s of the 4328.70s of remaining time.\n",
            "\t-1638737742045777.5\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t777.97s\t = Training   runtime\n",
            "\t25.34s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1745.30s of the 3523.73s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.64% memory usage per fold, 58.56%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=14.64%)\n",
            "\t-1502833927640651.2\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t471.66s\t = Training   runtime\n",
            "\t1.22s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1270.15s of the 3048.58s of remaining time.\n",
            "\t-1636585125122251.2\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t403.37s\t = Training   runtime\n",
            "\t23.14s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 841.83s of the 2620.26s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.70% memory usage per fold, 45.40%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=22.70%)\n",
            "\t-1527985257844115.2\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t357.72s\t = Training   runtime\n",
            "\t2.35s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 480.74s of the 2259.17s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.07% memory usage per fold, 72.28%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=18.07%)\n",
            "\t-1492979986042019.5\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t387.08s\t = Training   runtime\n",
            "\t5.52s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 89.92s of the 1868.35s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.90% memory usage per fold, 47.59%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=11.90%)\n",
            "\t-1144709403846416.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t74.58s\t = Training   runtime\n",
            "\t8.14s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 11.14s of the 1789.57s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.47% memory usage per fold, 65.87%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=16.47%)\n",
            "\t-2125533393260836.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t13.72s\t = Training   runtime\n",
            "\t0.3s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1772.92s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 1.0}\n",
            "\t-1144709403846416.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t0.37s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1772.53s of the 1772.09s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.95% memory usage per fold, 55.80%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=13.95%)\n",
            "\t-1491401199198195.0\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t90.23s\t = Training   runtime\n",
            "\t0.85s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1679.22s of the 1678.77s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.98% memory usage per fold, 55.90%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=13.98%)\n",
            "\t-1496422147269785.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t95.06s\t = Training   runtime\n",
            "\t0.73s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 1580.84s of the 1580.40s of remaining time.\n",
            "\t-1557756293449485.0\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t1013.03s\t = Training   runtime\n",
            "\t26.88s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ... Training model for up to 538.94s of the 538.50s of remaining time.\n",
            "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.10% memory usage per fold, 60.41%/80.00% total).\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=15.10%)\n",
            "\t-1508070878241807.2\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t55.33s\t = Training   runtime\n",
            "\t1.97s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 480.01s of the 479.57s of remaining time.\n",
            "\t-1562374222585857.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t513.63s\t = Training   runtime\n",
            "\t27.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -63.00s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 1.0}\n",
            "\t-1144709403846416.8\t = Validation score   (-mean_absolute_percentage_error)\n",
            "\t0.34s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 5403.17s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1074.5 rows/s (8750 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/seara/Desktop/Github/alfa-challenge/AutogluonModels/ag-20241216_132207\")\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "predictor.leaderboard()"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score_val</th>\n",
              "      <th>eval_metric</th>\n",
              "      <th>pred_time_val</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>pred_time_val_marginal</th>\n",
              "      <th>fit_time_marginal</th>\n",
              "      <th>stack_level</th>\n",
              "      <th>can_infer</th>\n",
              "      <th>fit_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NeuralNetTorch_BAG_L1</td>\n",
              "      <td>-1.144709e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>8.143130</td>\n",
              "      <td>74.580151</td>\n",
              "      <td>8.143130</td>\n",
              "      <td>74.580151</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WeightedEnsemble_L3</td>\n",
              "      <td>-1.144709e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>8.144419</td>\n",
              "      <td>74.925113</td>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.344963</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>-1.144709e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>8.145094</td>\n",
              "      <td>74.948440</td>\n",
              "      <td>0.001964</td>\n",
              "      <td>0.368289</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LightGBMXT_BAG_L1</td>\n",
              "      <td>-1.483280e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>17.337382</td>\n",
              "      <td>528.753602</td>\n",
              "      <td>17.337382</td>\n",
              "      <td>528.753602</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LightGBMXT_BAG_L2</td>\n",
              "      <td>-1.491401e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>123.789709</td>\n",
              "      <td>3512.466010</td>\n",
              "      <td>0.853635</td>\n",
              "      <td>90.232192</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost_BAG_L1</td>\n",
              "      <td>-1.492980e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>5.515049</td>\n",
              "      <td>387.080469</td>\n",
              "      <td>5.515049</td>\n",
              "      <td>387.080469</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LightGBM_BAG_L1</td>\n",
              "      <td>-1.493012e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>10.447278</td>\n",
              "      <td>405.630191</td>\n",
              "      <td>10.447278</td>\n",
              "      <td>405.630191</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LightGBM_BAG_L2</td>\n",
              "      <td>-1.496422e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>123.662307</td>\n",
              "      <td>3517.297616</td>\n",
              "      <td>0.726233</td>\n",
              "      <td>95.063799</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>CatBoost_BAG_L1</td>\n",
              "      <td>-1.502834e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>1.219507</td>\n",
              "      <td>471.658283</td>\n",
              "      <td>1.219507</td>\n",
              "      <td>471.658283</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CatBoost_BAG_L2</td>\n",
              "      <td>-1.508071e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>124.904857</td>\n",
              "      <td>3477.560842</td>\n",
              "      <td>1.968783</td>\n",
              "      <td>55.327025</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NeuralNetFastAI_BAG_L1</td>\n",
              "      <td>-1.527985e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>2.347853</td>\n",
              "      <td>357.721489</td>\n",
              "      <td>2.347853</td>\n",
              "      <td>357.721489</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>RandomForestMSE_BAG_L2</td>\n",
              "      <td>-1.557756e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>149.814680</td>\n",
              "      <td>4435.259165</td>\n",
              "      <td>26.878606</td>\n",
              "      <td>1013.025347</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ExtraTreesMSE_BAG_L2</td>\n",
              "      <td>-1.562374e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>149.951495</td>\n",
              "      <td>3935.859165</td>\n",
              "      <td>27.015421</td>\n",
              "      <td>513.625347</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ExtraTreesMSE_BAG_L1</td>\n",
              "      <td>-1.636585e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>23.138281</td>\n",
              "      <td>403.367777</td>\n",
              "      <td>23.138281</td>\n",
              "      <td>403.367777</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>RandomForestMSE_BAG_L1</td>\n",
              "      <td>-1.638738e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>25.337533</td>\n",
              "      <td>777.970011</td>\n",
              "      <td>25.337533</td>\n",
              "      <td>777.970011</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>KNeighborsUnif_BAG_L1</td>\n",
              "      <td>-1.858095e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>29.150266</td>\n",
              "      <td>1.750068</td>\n",
              "      <td>29.150266</td>\n",
              "      <td>1.750068</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LightGBMLarge_BAG_L1</td>\n",
              "      <td>-2.125533e+15</td>\n",
              "      <td>mean_absolute_percentage_error</td>\n",
              "      <td>0.299794</td>\n",
              "      <td>13.721777</td>\n",
              "      <td>0.299794</td>\n",
              "      <td>13.721777</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     model     score_val                     eval_metric  \\\n",
              "0    NeuralNetTorch_BAG_L1 -1.144709e+15  mean_absolute_percentage_error   \n",
              "1      WeightedEnsemble_L3 -1.144709e+15  mean_absolute_percentage_error   \n",
              "2      WeightedEnsemble_L2 -1.144709e+15  mean_absolute_percentage_error   \n",
              "3        LightGBMXT_BAG_L1 -1.483280e+15  mean_absolute_percentage_error   \n",
              "4        LightGBMXT_BAG_L2 -1.491401e+15  mean_absolute_percentage_error   \n",
              "5           XGBoost_BAG_L1 -1.492980e+15  mean_absolute_percentage_error   \n",
              "6          LightGBM_BAG_L1 -1.493012e+15  mean_absolute_percentage_error   \n",
              "7          LightGBM_BAG_L2 -1.496422e+15  mean_absolute_percentage_error   \n",
              "8          CatBoost_BAG_L1 -1.502834e+15  mean_absolute_percentage_error   \n",
              "9          CatBoost_BAG_L2 -1.508071e+15  mean_absolute_percentage_error   \n",
              "10  NeuralNetFastAI_BAG_L1 -1.527985e+15  mean_absolute_percentage_error   \n",
              "11  RandomForestMSE_BAG_L2 -1.557756e+15  mean_absolute_percentage_error   \n",
              "12    ExtraTreesMSE_BAG_L2 -1.562374e+15  mean_absolute_percentage_error   \n",
              "13    ExtraTreesMSE_BAG_L1 -1.636585e+15  mean_absolute_percentage_error   \n",
              "14  RandomForestMSE_BAG_L1 -1.638738e+15  mean_absolute_percentage_error   \n",
              "15   KNeighborsUnif_BAG_L1 -1.858095e+15  mean_absolute_percentage_error   \n",
              "16    LightGBMLarge_BAG_L1 -2.125533e+15  mean_absolute_percentage_error   \n",
              "\n",
              "    pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
              "0        8.143130    74.580151                8.143130          74.580151   \n",
              "1        8.144419    74.925113                0.001289           0.344963   \n",
              "2        8.145094    74.948440                0.001964           0.368289   \n",
              "3       17.337382   528.753602               17.337382         528.753602   \n",
              "4      123.789709  3512.466010                0.853635          90.232192   \n",
              "5        5.515049   387.080469                5.515049         387.080469   \n",
              "6       10.447278   405.630191               10.447278         405.630191   \n",
              "7      123.662307  3517.297616                0.726233          95.063799   \n",
              "8        1.219507   471.658283                1.219507         471.658283   \n",
              "9      124.904857  3477.560842                1.968783          55.327025   \n",
              "10       2.347853   357.721489                2.347853         357.721489   \n",
              "11     149.814680  4435.259165               26.878606        1013.025347   \n",
              "12     149.951495  3935.859165               27.015421         513.625347   \n",
              "13      23.138281   403.367777               23.138281         403.367777   \n",
              "14      25.337533   777.970011               25.337533         777.970011   \n",
              "15      29.150266     1.750068               29.150266           1.750068   \n",
              "16       0.299794    13.721777                0.299794          13.721777   \n",
              "\n",
              "    stack_level  can_infer  fit_order  \n",
              "0             1       True          9  \n",
              "1             3       True         17  \n",
              "2             2       True         11  \n",
              "3             1       True          2  \n",
              "4             2       True         12  \n",
              "5             1       True          8  \n",
              "6             1       True          3  \n",
              "7             2       True         13  \n",
              "8             1       True          5  \n",
              "9             2       True         15  \n",
              "10            1       True          7  \n",
              "11            2       True         14  \n",
              "12            2       True         16  \n",
              "13            1       True          6  \n",
              "14            1       True          4  \n",
              "15            1       True          1  \n",
              "16            1       True         10  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "predictions = predictor.predict(X_test)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "submission[\"target\"] = np.clip(predictions, 0, 7)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "submission.to_csv(\"submissions/500_features_autogluon_MAPE.csv\", index=False)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "submission[\"target\"] = submission[\"target\"] - 0.5"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [
        "submission.to_csv(\"submissions/500_features_autogluon_MAPE-0.5.csv\", index=False)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "source": [],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    }
  ]
}
